\section{Computer programs}

With the upgrade of synchrotrons to more brilliant sources, BCDI beamlines (ID01 - ESRF, P10 - PETRA III, SixS and CRISTAL - SOLEIL, NanoMAX - MAX IV, 34-ID-C - APS) have received increasing attention from the scientific community.
Imaging experiments yield larger and more complex data that far exceed the basic diffraction pattern (a reconstructed BCDI measurement consists of a complex array in 3 dimensions).
In a general frame, the increase in flux at synchrotrons leads to quicker experiments, which in turn leads to an increased amount of data stored on the beamlines, often in different formats.

Data produced at synchrotron has reached a volume and complexity that can nowadays be considered as part of \textit{big data} \parencite{Alizada2017, Wang2018}, referring to vast volumes, generated at high velocities, and from various sources.
Big data becomes too complex or large to be processed and analysed using traditional tools or methods, originally written to simulate or fit reduced amounts of data.
In the specific case of Bragg coherent diffraction imaging, new tools have been developed during this thesis (sec. \ref{sec:Gwaihir}, \cite{Carnis2021c, Simonne2022}) to help create a fast and reproducible workflow.
Nevertheless, it is a more general way of working that must be adopted in synchrotrons to adapt and profit from the transition to big data \parencite{Wang2018}.
This does not only concern scientific analysis, but also the complexity of operating synchrotron beamlines, that could benefit from this transition to upgrade their performances \parencite{Diadem}.

First, synchrotron data must be stored in a comprehensive format (e.g. \textit{NeXuS} format, KÃ¶nnecke et al \cite*{Konnecke2015}).
The stored files must not only contain detector images, but routinely include all of the important metadata (informational data) to provide a more comprehensive and detailed view of the experiments.

Secondly, synchrotrons must offer high performance computing (HPC) clusters \parencite{Wang2021}, and make the data available to their users through an interface compatible with the most common integrated development environment (IDE).

Finally, synchrotrons must either have an internal data analysis strategy, or trust their users to be able to upgrade and develop tools that, in a first stage, are able to handle large volumes of data.
In a second stage, those tools can take advantage from that volume to uncover valuable insights \parencite{Wang2016a, Khaleghi2019}.
These insights can lead to new discoveries and enhance our understanding of complex phenomena, driving scientific progress.

% The use of big data expands research capabilities by allowing studies to be conducted on a much larger scale whereas traditional scientific experiments often rely on a limited number of observations or samples, which may not capture the full complexity of real-world phenomena.

\subsubsection{Result reproducibility}

At synchrotrons, where experimental techniques become ever more complicated, the first step of result reproducibility consists in the capacity for scientists to replicate experiments while obtaining consistent data.
The data that stems from the experiment is also sometimes called \textit{raw data}, which is then \textit{reduced} (ordered and simplified, \textit{e.g.} by phase retrieval in BCDI or integration in SXRD) and \textit{analysed} to derive scientific \textit{results}.
The second step refers to the ability to replicate or re-run the data reduction and analysis processes from the raw data to obtain similar results.
A \textit{dataset} is then defined as a collection of data, \textit{e.g.} in BCDI a dataset is constituted by the raw data, the detector mask, the measurement metadata, the retrieved Bragg electronic density, the displacement and strain arrays, and the data reduction parameters.

Achieving reproducibility with big data can be challenging due to the massive volume, velocity, variety and complexity of the data involved.
However, linking reproducibility and big data is crucial for ensuring the credibility and reliability of the insights derived from large and complex datasets.

For example, it is possible to measure a single rocking curve in less than a minute at specialised BCDI beamlines, such as ID01 at the upgraded European synchrotron \parencite{Leake2019}.
The high complexity of the data reduction and analysis process in BCDI makes the replication of result complex for external scientists, that often do not have access to the same data reduction softwares, or to the value of the parameters used during the process.

Discussions about defining a set of rules that regulate research practice \parencite{Kretser2019}, and reduce the grey zone that includes scientific misconduct at all levels of academia \parencite{Kornfeld2016} are growing, raising awareness on reproducibility in the scientific community.

Staggering numbers \parencite{Baker2016} show that about \qty{65}{\percent} of scientists in the field of physics and engineering struggle to reproduce others' results, and about \qty{50}{\percent} fail to reproduce their own results.
These numbers can sometimes be linked to very precise environments and techniques, with experimental conditions and processes difficult to replicate between different laboratories, or to knowledge transfer from academia to industry \parencite{Sarwitz2015}.
However, according to the same study \parencite{Baker2016}, code availability, insufficient peer reviewing, and access to raw data contribute together to this issue.
Moreover, it is largely accepted that the publishing industry has its own role to play by facilitating peer-reviewing to promote reproducibility \parencite{Lee2017}.

Therefore, code-availability, access to raw data combined with metadata, and well-defined workflows are goals of utmost importance for experimental science \parencite{Munafo2017}.
Reproducible results lead to a global improvement of confidence in new techniques, such as BCDI, which could subsequently result in growth of interest and community.
Key problems are for example the difficulty to reconstruct highly strained objects, or the determination of a metric that would allow scientists to \textit{blindly} trust reconstructed data, permitting a fully automatised data reduction workflow.
Reproducibility is first permitted by the use of a common data reduction and analysis environment.

\subsubsection{\textit{Python} in the \textit{Jupyter} environment}

The \textit{Jupyter Notebook} environment \parencite{Perez2007, Kluyver2016} was chosen for the development of data reduction and analysis tools during this thesis for its versatile, user-friendly and browser-based interface.
Notebooks can be used to take notes during experiments, shared in the \textit{.ipynb} format or as \textit{.pdf} documents.
They allow the use of \textit{Python}, an accessible programming language that has gradually become one of the most popular, versatile \parencite{Perez2007, Behnel2011, Newville2016, Ronaghi2017}, and widely-taught \parencite{Ayer2014, Scopatz2015, McKinney2017, Boulle2019} programming languages in science.

Moreover, \textit{Jupyter Notebook} has proven to be an effective tool for the reduction and analysis of synchrotron data in terms of graphical user interface (GUI) \parencite{Martini2019a,Simonne2020}, but also in terms of supporting scientific communities looking for high-performance frameworks \parencite{jupyter_computing_4, jupyter_computing_1, jupyter_computing_3, jupyter_computing_2}.

Large scale facilities and institutions seek ways to provide remote-access to high performance computing services for their users, which combine existing solutions in an interactive and user-friendly environment.
To simplify the data reduction and analysis pipelines in fourth-generation synchrotrons, it is of critical importance to offer the possibility for external users to analyse raw data remotely, with access to computational environments.
\textit{Jupyter} (\url{https://jupyter.org/}) is particularly advantageous and was chosen by several institutions for such purposes, \textit{e.g}. Google (Google Colab), the EGI federation, or the European Synchrotron.

Remote access to high performance computational environments, interfaced with \textit{Jupyter Notebook} or \textit{JupyterLab}, is provided by \textit{JupyterHub}.
For example, specific hardware such as graphical processing units (GPUs), mandatory for accelerated phase retrieval with \textit{PyNX}, can be managed with \textit{JupyterHub}.
Researchers can create their own work-spaces, with direct access to tailored computational environments, while relying on system administrators that can efficiently manage complex environments accessible for all users.
Remotely accessing data avoids storage issues, which can quickly become problematic with current experiments.
Note that in the latest version \parencite{JupyterNotebook7}, real-time collaboration (RTC) will be supported, which also pushes \textit{Jupyter Notebook} forward as a tool for laboratory notebooks.

\subsection{\textit{Gwaihir}} \label{sec:Gwaihir}

BCDI relies on iterative algorithms to solve the phase lost during the measurement (\cite{Robinson2009}).
A 3D intensity distribution in the vicinity of a Bragg peak (stack of diffraction patterns forming a 3D reciprocal space map with the proper sampling) is collected from a sample illuminated with coherent light \parencite{Robinson2005}, and serves as input for phase retrieval.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.66\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/Packages.png}
    \caption{
    Flow chart illustrating the main steps in the BCDI data reduction workflow.
    \textit{Gwaihir} links the \textit{bcdi} and \textit{PyNX} packages \textit{via} its graphical user interface and command line scripts, resulting in a complete and understandable workflow.
    Opt. stands for optional.
    }
    \label{fig:Packages}
\end{figure}

There are three main steps to compute the strain, detailed in fig. \ref{fig:Packages}.
The raw data must first be pre-processed.
It can then be inverted \textit{via} phase retrieval, the phase containing structural information about the sample that are lost during the measurement.
Finally, it is possible to extract information on the physical state of the particle such as its shape or internal strain from the images after post-processing.
Several software packages were developed to solve these steps but none bring a comprehensive pipeline from start to finish.

For example, \textit{PyNX} \parencite{FavreNicolin2011} focuses on the phase retrieval step, \textit{bcdi} \parencite{Carnis2021c} on data pre-processing and post-processing, while \textit{Cohere} \parencite{Frosik2021} focuses on pre-processing and phase retrieval.
A few graphical user interfaces also exist, such as \textit{Cohere} \parencite{Frosik2021}, \textit{Phasor} \parencite{Dzhigaev2021}, and \textit{Bonsu} \parencite{Newton2012}.
Providing a workflow will reduce the time spent on data reduction for newcomers, and improve results reproducibility by facilitating sharing while keeping track of parameters and metadata.

\textit{Gwaihir} is a tool developed during this thesis, which brings together the \textit{PyNX} and \textit{bcdi} packages, binding them in a graphical user interface built for the \textit{Jupyter} framework \parencite{Kluyver2016}.
It provides an interface to the bleeding edge of data reduction in BCDI, and can be used both locally or remotely, offering an interactive and user-friendly interface with complex functionality satisfying both beginners and experts.

\textit{Gwaihir} works with \textit{Python} 3.9 and is licensed under the GNU General Public License v3.0.
The source code as well as the latest developments are available on GitHub, while each stable version is released on the \textit{Python} Package Index (PyPi), along with its documentation.

\subsubsection{Workflow for Bragg coherent diffraction imaging} \label{sec:Workflow}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.7\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/Workflow.png}
    \caption{Workflow steps taken in \textit{Gwaihir}; the circular workflow illustrates result reproducibility, a key concept, facilitated by using the \textit{cxi} architecture.}
    \label{fig:Workflow}
\end{figure}

\textit{Gwaihir} offers an interactive workflow meant to be reproducible, and resulting in the phase and amplitude of the probed object (fig. \ref{fig:Workflow}).
To illustrate this, the results of the following procedure on a dataset collected at the P10 beamline in at PETRA III are shown in fig. \ref{fig:GUI_file} (CXI dataset ID 195).

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.49\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/CropDiffPatternRed.png}
    %\includegraphics[width=\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/CropObjectAmplutideRed2.png}
    \includegraphics[width=0.49\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/crop_phase_red.png}
    \caption{
    A 2D slice of a 3D coherent diffraction pattern is shown (left).
    The phase of the Bragg electronic density (\unit{\radian}) of the reconstructed object is shown (right), the facets are clearly visible.
    Image displayed \textit{via} \textit{JupyterHub}.
    The particle is \qty{300}{\nm} wide.
    }
    \label{fig:GUI_file}
\end{figure}

\subsubsection{Pre-processing} \label{sec:preprocess}

Data pre-processing aims at improving the quality of phase retrieval by optimising the size and content of the 3D array used as input \parencite{Ozturk2017}.
The minimal processing consists in loading the raw data and stacking it together as an input for the phase retrieval.
Intermediate optional steps can be added \textit{via} a YAML (YAML Ain't Markup Language) configuration file.

Once the raw data is collected, different pre-processing parameters can be modified to optimise the 3D diffraction intensity.
For example, the array must be centred for the Fourier transforms, the centre is either fixed manually as the centre of the Bragg peak if known, or determined as the centre of mass of the 3D array during the reduction process.
The array can also be cropped to reduce its size and decrease the computing time during phase retrieval by removing the points furthest from the Bragg peak where the signal-to-noise ratio is low (fig. \ref{fig:GUI_file}).

It is important to create a detector mask prior to the experiment to correct the raw data for hypothetical hot-pixels and uneven background.
Moreover, it is also possible to correct the raw data image by image for spurious data which taints the diffraction pattern.
For example, it is possible, while recording the 3D diffracted intensity of a given reflection, to have signal coming either from the substrate or from neighbouring objects that will be summed to the probed object's intensity.

Finally, it is possible to normalise the raw data by an intensity monitor, or compute the scattering vector $\vec{q}$ of the measurement, from the instrumental geometry and parameters.

Aside from the specific details of the experimental setup (diffractometer and setup geometry, detector type, file system), the majority of the data reduction process is beamline-independent \parencite{Yang2019a}.
Based on this observation, \textit{bcdi} leverages inheritance and transforms the raw data to a common data format used for phase-retrieval (fig. \ref{fig:Packages}).
It frees the user from having to learn or remember the technical details for each beamline and sets a common strategy across beamlines.

\subsubsection{Phase retrieval} \label{sec:phaseretrievalpynx}

To retrieve the phase from the diffracted intensity, \textit{PyNX} functions with different iterative algorithms \parencite{Gerchberg1972, Fienup1982, Fienup1978, Marchesini2007, FavreNicolin2020}.
\textit{PyNX}'s recent update includes mathematical operators \parencite{FavreNicolin2020} which represent most of the reconstruction operations traditionally used in phase retrieval, and is at the foundation of quick and interactive phase retrieval in \textit{Gwaihir}.
Indeed, state-of-the-art GPUs available onsite where data collection occurs enable almost real time data reduction.
This results in quick visualisation of the probed object's amplitude and phase, offering the possibility to optimise data acquisition during the experiment.
Such live feedback is critical to the success of an experiment, and provides broader opportunities in the type of experiments that can be carried on.

The different parameters such as e.g. the support threshold, the number of iterations for each algorithm, the object initialisation procedure (square, sphere, auto-correlation, ...) can be modified in the GUI (fig. \ref{fig:PRT}).
Initial guesses are given for each parameter but must be refined by the user to optimise the results.
The impact of each parameter can be viewed directly in the GUI after phase retrieval.
This allows to refine the phase retrieval input parameters before submitting a \textit{batch job}, that will spawn a sub-process on the computing cluster for phase retrieval.
This highly optimised procedure can yield dozens of solutions in a few minutes.

With well-tuned parameters and high quality datasets, phase retrieval converges towards the same solution, but with minor differences between each reconstructed object, related to the phase retrieval process.
The most important limiting factors for the convergence of the reconstruction algorithms are the quality of the measurement, and the amount of strain going through the particle.
In the case of highly strained object, part of the intensity will be scattered in a direction that does not fit the area collected by the detector, resulting in void areas inside the reconstructed electronic density.
Moreover, the support becomes very difficult to determine since the diffraction intensity does not correspond to the lattice factor anymore.

\begin{figure}[!htb]
    \centering
    %\includegraphics[width=\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/phase_retrieval_tab.png}
    \includegraphics[width=\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/PRTab.pdf}
    \caption{
    Phase retrieval tab in \textit{Gwaihir}.
    Parameters are separated into groups (files, support, point-spread function, algorithms, ...) and detailed in the \textit{Readme} tab.
    The object can be reconstructed through a batch job, submitted to the computing cluster in the backend, or with operators, that will plot the evolution of the reconstruction object in the Notebook.
    }
    \label{fig:PRT}
\end{figure}

\textit{Gwaihir} provides a wide range of selection criteria to find the best solution.
Each reconstructed object has a list of final attributes that can be used as a criterion for selection, such as the free log-likelihood \parencite{FavreNicolin2020a} or the standard deviation of the modulus of the reconstructed object.
In the case of crystallographic defects, specific metrics (Chi, Sharp, Max volume, ...) were derived that perform best depending on the type of defect in the object \parencite{Ulvestad2017}.
Following the selection criterion, it is possible to quickly identify the solutions of poor quality that must be ignored to create a set of best solutions.
\textit{PyNX} then offers a method that merges this set into a single solution by computing eigen-vectors for the selected solutions \parencite{FavreNicolin2020}.
An alternative approach also available in \textit{Gwaihir} is to take the average of the best solutions \parencite{Ulvestad2014}.

\subsubsection{Post-processing} \label{sec:postprocess}

Post-processing regroups methods applied to the complex output of the phase retrieval.
Once the solution with the best figure of merit is selected, it is possible to use the \textit{bcdi} scripts to process the object.
If the data is still in the detector frame (geometric transformation not applied during pre-processing), the data can be interpolated in the orthogonal laboratory frame ($\vec{z}$ downstream, $\vec{y}$ vertical up, $\vec{x}$ outboard) or in the sample frame ($\vec{z}$ out-of-plane, $\vec{y}$ perpendicular to the beam, $\vec{x}$ parallel to the beam) using a transformation matrix.
This allows an easier comparison between the object's evolution when probing different Bragg reflections \parencite{Lauraux2021}.
The correct beamline and instrumental parameters must be selected (\textit{e.g.} sample-detector distance, probing energy, detector pixel size, \textit{etc}...), usually constant throughout the experiment.
The geometric transformation can be realised using either the transformation matrix \parencite{Mark2005} or the \textit{xrayutilities} package \parencite{Kriegner2013}, depending on which reference basis is needed.

After phase unwrapping, a refraction and absorption correction are optionally applied, and possible phase ramp and phase offset are removed.
At this point the displacement and the strain component are calculated from the phase.
The phase origin can first have an impact when comparing the lattice displacement between different reconstructions \parencite{Atlan2023}.
In the case of weak strain, it can be sufficient to consider the centre of mass of the object as the origin of phase.
However, this can become quite complex in the case of defects or defaults in the object.
Special methods are defined to target this issue in \textit{bcdi}.
For example, Guizar-Sicairos et al. \parencite*{GuizarSicairos2011} and Hofmann et al. \parencite*{Hofmann2020} proposed a convenient method for the numerical calculation of phase gradients in the presence of phase jumps.

\subsubsection{Graphical interface}

\textit{Gwaihir} links in a unique user-friendly and interactive GUI the aforementioned packages, whilst offering 2D/3D browser-based data visualisation tools.
Therefore, the main purpose of \textit{Gwaihir}, is to offer a possibility for the user to work solely in the \textit{Jupyter} environment.
The graphical user interface is divided into 10 tabs, aiming to achieve a comprehensible but fluid workflow, whilst still separating each step.
A final \textit{Readme} tab contains information about the different methods and parameters used in the workflow, as well as a tutorial on the GUI.

\textit{Jupyter} natively offers multiple options for interactive data plotting.
Most of the figures displayed in the GUI are based on the \textit{matplotlib} package \parencite{Hunter2007}.
For example, it is possible to select a list of 3D complex data arrays, and to visualise 2D slices in each dimension of their amplitude or phase, with different colour-maps (diverging, sequential, cyclic) and scale (linear, logarithmic).

\begin{figure}[!htb]
    \centering
    %\includegraphics[width=\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/3D_obj.png}
    \includegraphics[width=\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/PlotTab.pdf}
    \caption{
    Visualisation tab in \textit{Gwaihir}.
    The selection of the data array, the colormap, and the contour of the resulting 3D object are performed through different widgets.
    }
    \label{fig:3D_object}
\end{figure}

Moreover, an interactive 3D visualisation tool is provided that relies on the \textit{ipyvolume} library \parencite{Breddeld2021}, itself built on top of \textit{ipywidgets}, and specifically designed to quickly render large 3D data arrays.
In the specific case of single volume rendering for the reconstructed object, the object's surface is defined by a threshold of its maximum density (fig. \ref{fig:3D_object}).
The object surface can be colour-mapped with the values of the displacement and strain retrieved during the data reduction.

More complex interaction with figures and images (\textit{e.g.} zoom, set the colorbar range) is implemented with \textit{Bokeh} \parencite{Bokeh}, a \textit{Python} library that transforms figures in interactive web-pages (fig. \ref{fig:BokehDetector}), that can also be displayed in \textit{Jupyter Notebook}.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.66\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/detector_slice_bokeh.png}
    \caption{
    Detector images can be viewed interactively with \textit{Bokeh}, to zoom on the data, and visualise, for example, the intensity collected on each pixel.
    }
    \label{fig:BokehDetector}
\end{figure}

\subsubsection{Command line scripts}

The complete workflow for data processing can also be launched from the command-line using \textit{Python} scripts, whereas the link between each package is based on \textit{BASH} scripts.
This approach is both fast and versatile, designed to quickly iterate on several datasets to test parameters values, but less intuitive compared to the interactive GUI.

A set of default parameters stored in a configuration file is used, that can also be overwritten by directly providing keywords directly in the command line.
For example, the same file can be used for different measurements by just changing the scan number in the command.
The configuration files are written in \textit{YAML} (fig. \ref{fig:YAML_file}).
% One may still manually mask the data, or manually select the best reconstruction before post-processing.
% This option allows a quick visualisation of the probed object.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/yaml.png}
    \caption{
    Configuration file in YAML, a human-readable data-serialisation language with a minimalist syntax.
    A configuration file is generated for the pre-processing and post-processing scripts, as well as for the phase retrieval.
    }
    \label{fig:YAML_file}
\end{figure}

\subsubsection{The CXI database}

Raw and processed data can both be accessed for the BCDI technique through the \textit{.cxi} database (\url{https://cxidb.org/}, Maia \cite*{Maia2012}), which aims at creating a single data-storing architecture/format for coherent X-ray imaging experiments.
The data input/output follows Nexus definitions \parencite{Konnecke2015}.
Both clarity and consistency in data formatting encourages the reproducibility of the science produced (\url{https://www.panosc.eu/}) and guarantees the workflow.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.7\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/tree.png}
    \caption{
    Each parameter value used during the workflow is stored in the same \textit{.cxi} file, along with the results, in a nested architecture, displayed \textit{via} \textit{JupyterHub}.
    \text{instrument\_1} regroups parameters associated with the instrumental setup.
    \text{data\_1} is the collected diffraction pattern.
    \text{image\_2} regroups the parameters associated with phase retrieval.
    \text{data\_2} is the reconstructed Bragg electronic density chosen for post-processing.
    \text{image\_3} regroups parameters linked to data processing, as well as the processed amplitude, phase and resulting strain.
    \text{data\_3} is a link to the processed phase of the object.
    }
    \label{fig:TREE}
\end{figure}

Since not all beamlines provide self-explaining NeXuS datasets, it is the \textit{bcdi} package together with \textit{xrayutilities} \parencite{Kriegner2013} that allows the support of most of the coherent imaging beamlines: ID01 (ESRF), P10 (PETRA III), SixS and CRISTAL (SOLEIL), NanoMAX (MAX IV) and 34-ID-C (APS).
Data pre-processing will generate two files stored as NumPy arrays \parencite{VanDerWalt2011}, corresponding to the diffraction intensity and mask.
These two files are then used for phase retrieval, for which the final object is saved in a \textit{cxi} file, later used for data post-processing.
In the case of simulation, the simulated diffraction intensity can be stored as a NumPy array to start the workflow from phase retrieval.

In \textit{Gwaihir}, results sharing across teams and team members is facilitated by the creation of a single output file, respecting the \textit{CXI} \parencite{Maia2012}, and thus the NeXuS \parencite{Konnecke2015} architecture (fig. \ref{fig:TREE}), which pushes towards results reproducibility.

Key parameters are the transformation matrix used for the interpolation in the final frame, the voxel size of the resulting 3D array, the probed reciprocal space range after data pre-processing $(\delta q_x , \delta q_y , \delta q_z)$, the iso-surface threshold, \textit{etc}.
Comments or metadata, such as the horizontal and vertical coherence lengths, or beam size, if determined prior to the experiment, can also be saved.
The aim is first to have a comprehensible architecture, and secondly to be able to reproduce anyone result from a \textit{single file}.
On a small scale, results are easier to share between collaborators and more understandable, while on a larger scale peer-review is facilitated.

\subsection{BINoculars} \label{sec:BINoculars}

BINoculars \parencite{Roobol2015} is a data reduction software used to concatenate the scattering intensity from successive angular scans.
It can also be used to optionally change the data's reference frame (e.g. from angular space to $q$ space).
Those two steps can be performed simultaneously by assigning to each voxel of the 3D scattered intensity a position in the new frame.
From subsequent scans, it is possible that each position in the new frame was scanned multiple times, meaning that there were multiple contributions (possibly with different intensity attenuation correction) to the total scattered intensity at that position.
The correct intensity is therefore the division of the total scattering intensity in each voxel by the number of contribution to that voxel (eq. \ref{eq:BinocularsIntensity}).
If there are no contributions, the intensity is set to NaN (Not a Number).

\begin{equation}
    \label{eq:BinocularsIntensity}
    Intensity =
        \begin{cases}
            counts \, /  \,contributions  & \text{if contributions $\neq$ 0} \\
            NaN & \text{if contributions = 0}
        \end{cases}
\end{equation}

Different orthonormal frames can be of interest when analysing the data, the q-space ($q_x, q_y, q_z$) is useful to index Bragg peaks and to access the related inter-reticular spacing $d_{hkl}$.
Different lattices can be used to highlight the difference of geometry between the bulk structure and the surface structure (fig. \ref{fig:MapExampleBinoculars}).

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.66\textwidth]{/home/david/Documents/PhDScripts/SixS_2022_01_SXRD_Pt100/figures/Map_hkl_surf_or_2227-2283_patched.pdf}
    % \includegraphics[width=0.49\textwidth]{/home/david/Documents/PhD/Figures/sxrd_data/Pt100/maps/Map_hkl_hex_or_2227-2283_not_patched.png}
    \caption{
    Reciprocal space in-plane maps.
    The large high intensity peaks are Bragg peaks from the bulk structure, the low intensity peaks are from surface structures.
    The map is computed using a cubic unit cell, two hexagonal unit cells corresponding to the surface structures are drawn as diamonds.
    }
    \label{fig:MapExampleBinoculars}
\end{figure}

The reciprocal space in-plane maps collected during surface x-ray diffraction experiments are usually projected perpendicular to $l$ (or $q_z$), to show possible Bragg peaks from surface reconstruction (fig. \ref{fig:MapExampleBinoculars}).
They contain the sum of the scattering intensity in a small out-of-plane layer $\delta l$ (or $\delta q_z$).
It is of utmost importance to select a thin layer to not drown the surface signal with the background or bulk signal.

BINoculars also contains a sub-module (\textit{fitaid}) that allows the integration of the scattered intensity in the reciprocal space as a function of one axis.
Usually, to study the evolution of crystal truncation rods, the data is projected either in the ($h, k, l$) or ($q_x, q_y, q_z$) frame and then integrated in a square area around the CTR signal as a function of $l$ or $q_z$ (fig. \ref{fig:BinocularsBackground}).

The voxel size in each direction is set through a parameter, which raises a compromise between array size, resolution, and an interpolation process.
Moreover, to compare different maps, it is better to use the same resolution so that the intensities of the different signals are on the same scale.

When in the ($q_x, q_y, q_z$) space, it is possible to go as low as \qty{0.005}{\angstrom^{-1}} in each direction, the limit being set by instrumental parameters such as the detector pixel size, the amount of angular scans performed, the counting time, or the number of iteration per scans (and by the computer memory when computing or visualising the final space).

If the number of iteration is too low for the desired resolution, the process will yield an image with empty areas corresponding to voxels where no data could be recorded.
This is often the case during the collection of crystal truncation rods, at high values of $l$, since the region probed by the detector becomes thinner in $l$ with every iteration (sec. \ref{sec:DataCollectionSXRD} - \cite{Drnec2014}).
To avoid having to rely on an interpolation to fill these voxels, it is better to use a larger resolution when concatenating the images, and then to use the same step when integrating the data.

\begin{figure}[!htb]
    \includegraphics[width=\textwidth]{/home/david/Documents/PhD/Figures/sxrd_data/background_fitaid.png}
    \caption{
        Graphical user interface for the \text{fitaid} module of BINoculars.
        The CTR structure factor is computed using eq. \ref{eq:BinocSF} in a rectangular region of interest around the CTR signal.
        Four rectangular regions of interests around the signal are used to compute the background.
    }
    \label{fig:BinocularsBackground}
\end{figure}

To quantitatively compare the integrated intensity with simulated structure factors, it is mandatory to subtract the background intensity from the total intensity.
The background is calculated by taking the sum of all the values of selected background regions (fig. \ref{fig:BinocularsBackground}), around the CTR, corrected by the number of voxels in those area.
The average background value per voxel is then subtracted to each voxel in the CTR region of interest (ROI), the structure factor $F$ can then be computed as follows:

\begin{equation}
    |F|^2 \propto I_{roi} - \frac{N_{roi}}{N_{bkg}} \times I_{bkg}
    \label{eq:BinocSF}
\end{equation}

$I_{roi}$ is the integrated intensity in the ROI, $N_{roi}$ the number of voxels in the ROI, $I_{bkg}$ the integrated background intensity, and $N_{bkg}$ the number of voxels in the background.
% This becomes more complicated when there are additional signals such as powder rings that cross the CTR at different $l$ values.

A very fine resolution (e.g. \numproduct{0.005 x 0.005 x 0.05}) will result in extremely large arrays that can be difficult to manipulate on low resources computer.
For example, the maps shown in fig. \ref{fig:MapExampleBinoculars} have a three dimensional shape equal to (\numproduct{560 x 953 x 39}) which results in \num{2.1e7} voxels in the array.
Originally, this corresponds to the concatenation of 40 angular scans, each with 1010 steps, \textit{i.e.} containing 1010 2D detector images that have a shape equal to (\numproduct{240 x 560}).
This represents \num{5.5e9} pixels to process together to create such in-plane maps.
High performance computing clusters are therefore needed for the reduction of surface x-ray diffraction data with BINoculars, the data analysis is performed in a second step with programs such as \textit{ROD} \parencite{Vlieg2000}.

\subsection{\textit{ROD}} \label{sec:ROD}

Part of the aim of SXRD data analysis can be to comprehend the 3D structure of potential surface layers on top of the sample surface, as well as the magnitude of surface relaxations on its topmost layers.
Such information can be extracted by combining in-plane reciprocal space maps and crystal truncation rods, \textit{i.e.} by comparing the position and intensity of the scattered intensity (and structure factor) in reciprocal space with simulated data.

The primary challenge associated with surface x-ray diffraction lies in the complexity of data analysis, as researchers are limited to a small portion of the reciprocal space due to the extended data acquisition time \parencite{Gustafson2014}.
On one hand, a fine sampling of the reciprocal space will take hours which is hardly compatible with time-resolved experiments.
On the other hand, skipping areas of the reciprocal space could result in missing critical information if the symmetry of the surface structure is not known prior to the experiment.
For high symmetry structures, a small portion of the reciprocal space can sometimes be sufficient to have a final solution.
The typical approach for time-resolved experiments is to follow the intensity of a surface signal as a function of time.

\textit{ROD} is a computer program written by Vlieg et al. \parencite*{Vlieg2000}, that can be used qualitatively to simulate structure factors from a given structure, and quantitatively to refine the atomic positions within the structure by fitting the structure factors with the experimental data.
Since \textit{ROD} is written in \textit{C} and does not come with a graphical user interface, alternatives have been developed such as GenX \parencite{Bjorck2007, Glavic2022} written in \textit{Python} which mostly focuses on reflectivity.
In the frame of this thesis, \textit{ROD} was chosen for the analysis of SXRD data.

\begin{SCfigure}
    \centering
    \includegraphics[trim=0 1cm 0 1cm, clip, width=0.35\textwidth]{/home/david/Documents/PhD/Figures/introduction/Pt3O4.pdf}
    \caption{
        \ce{Pt_3O_4} bulk unit cell.
        Platinum atoms are situated on the faces on the cubic unit cell (e.g. $(0, 1/2, 1/4)$, $(0, 1/2, 3/4)$), while the eight oxygen atoms are inside the unit cell at the positions $(1/4, 1/4, z)$, $(1/4, 2/4, z)$, $(2/4, 1/4, z)$, $(2/4, 2/4, z)$ for $z=1/4$ and $z=3/4$.
    }
    \label{fig:Pt3O4_ROD}
\end{SCfigure}

The structure of our crystal must be understood as follows when working with surfaces, infinite in both in-plane dimensions, finite in the out-of-plane dimension.
The origin of the out-of-plane axis, commonly denoted $\vec{z}$, is situated at the surface of the crystal.
When $z$ is negative the \textit{bulk} structure of the crystal is described, whereas when $z$ is positive, the \textit{surface} structure of the crystal is described, usually only a few atomic layers thick where e.g. surface relaxation and reorganisation effects can be detected.

\begin{figure}[!htb]
    \includegraphics[width=\textwidth]{/home/david/Documents/PhDScripts/SixS_2022_01_SXRD_Pt100/simulations/rod_pt304/figures/rod_4_0_pt3o4.pdf}
    \caption{
        Simulation of out-of-plane structure factors computed with \textit{ROD}.
        Different layers within a single unit cell of \ce{Pt_3O_4} are used in (a), while a different amount of the same \ce{Pt_3O_4} unit cell is used in (b).
        Contribution from \ce{Pt3O4} are most important in the anti-Bragg region.
    }
    \label{fig:SimROD}
\end{figure}

Additional surface layers can be present on top of the main crystal structure, the example of a \ce{Pt_3O_4} platinum oxide is presented in fig. \ref{fig:SimROD}.
At least two layers in $\vec{c}$ are necessary to see a modulation of the out-of-plane signal, the Bragg peaks linked to the presence of a bulk oxide start to be clearly visible after 4 unit cells of \ce{Pt_3O_4} are present on the platinum surface.
The unit cell of \ce{Pt3O4} is presented in fig. \ref{fig:Pt3O4_ROD}.

In a second step, it is possible to refine the position of each atom in the unit cell to take into account possible strain along $\vec{c}$, that will have a significant effect in the shape of the crystal truncation rods as shown previously in fig. \ref{fig:CTRSimulation}.
The structure along $\vec{c}$ is refined by computing the shape of crystal truncation rods, and comparing them with experimental data.
The more rods measured, and the higher the extent of each rod in $L$, the more accurate the final structure.
