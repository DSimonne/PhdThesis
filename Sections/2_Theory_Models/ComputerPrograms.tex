\section{Computer programs}

With the upgrade of synchrotrons to more brilliant sources, imaging beamlines (ID01 - ESRF, P10 - PETRA III, SixS and CRISTAL - SOLEIL, NanoMAX - MAX IV, 34-ID-C - APS) have received increasing attention from the scientific community and yield larger and more complex data that far exceed the basic diffraction pattern (a reconstructed BCDI measurement consists of a complex array in 3 dimensions).
In a more general frame, the increase in flux at synchrotrons leads to quicker experiments, which in turn leads to an increased data stream stored on the beamlines, often in different formats.

Synchrotrons have effectively stepped in the wold of "big data" \parencite{Alizada2017, Wang2018}, which refers to a vast volume of data that is generated at a high velocity and comes from various sources.
This data is typically too complex or large to be processed and analyzed using traditional data management tools or methods that were originally written to simulate or fit only a few datasets at a time.
In the specific case of Bragg coherent diffration imaging, new tools have been developed during the thesis (sec. \ref{sec:Gwaihir}, \cite{jerome_carnis_2021_5741935, Simonne2022}) to help create a fast and reproducible workflow.
Nevertheless, it is a more general way of working that must be adopted in synchrotrons to adapt and profit from the transition to big data \parencite{Wang2018}.
This does not only concern scientific analysis but also the complexity of synchrotron beamlines that could benefit from the use of big data to upgrade their performance (CITE MAXIME PROJECT).

First, synchrotron data must be stored in a comprehensive data format \parencite{Konnecke2015} that contains not only detector images but all of the important metadata linked to the experiment, providing a more comprehensive and detailed view.
The fine selection of the beamline attributes that can be of interest for the discovery of hidden patterns, correlations, and trends that may not be apparent with smaller datasets is left to the beamline scientists.

Secondly, synchrotrons must offer high performance computing (HPC) clusters \parencite{Wang2021} and render the data available to the users through an interface compatible with the most common integrated development environment (IDE).

Finally, synchrotrons must either have an internal strategy or trust their users to be able to upgrade and develop tools that in a first stage are able to handle large volumes of data and in a second stage take advantage from that volume to uncover valuable insights \parencite{WangIEEE2016, Khaleghi2019}.
These insights can lead to new discoveries and enhance our understanding of complex phenomena, driving scientific progress.

% The use of big data expands research capabilities by allowing studies to be conducted on a much larger scale whereas traditional scientific experiments often rely on a limited number of observations or samples, which may not capture the full complexity of real-world phenomena.

\subsection{Data reproducibility}

On one hand, linking reproducibility and big data is crucial for ensuring the credibility and reliability of the insights derived from large and complex datasets.
Reproducibility in the context of big data refers to the ability to replicate or re-run data analysis processes to obtain consistent results.
Achieving reproducibility with big data can be challenging due to the massive volume, velocity, and variety of the data involved.
For example, in the case of Bragg coherent diffraction imaging, if measurements can take up to an hour at $3^{rd}$ generation synchrotrons such as SOLEIL, it is now possible to measure a single rocking curve in less than a minute at specialized beamlines such as ID01 at the upgraded European synchrotron \parencite{Richter2019}.
%In that case, in order to maximize the output of experiments carried out at synchrotrons

On the other hand, discussions about defining a set of rules that regulate research practice \parencite{Kretser2019} and reduce the grey zone that includes scientific misconduct at all levels of academia \parencite{Kornfeld2016} are growing, raising awareness on data reproducibility in the scientific community.

Staggering numbers \parencite{Baker2016} show that about \qty{65}{\percent} of scientists in the field of physics and engineering struggle to reproduce others' results, and more than \qty{50}{\percent} fail to reproduce their own results.
These numbers can sometimes be linked to very precise environments and techniques, with experimental conditions and processes difficult to reproduce between different laboratories, and also to knowledge transfer from academia to industry \parencite{DanielSarwitz2015}.
However, according to the study  \parencite{Baker2016}, code availability, insufficient peer reviewing, and access to raw data contribute to non-reproducible research.
Moreover, it is largely accepted that the publishing industry has its own role to play by facilitating peer-reviewing to promote reproducibility \parencite{Lee2017}.

Therefore, code-availability, access to raw data combined with metadata, and reproducible workflows are goals of utmost importance for experimental science \parencite{Munafo2017}.
Reproducible data will result in a global improvement of confidence in new techniques, such as BCDI, which could subsequently result in growth of interest and community.
Key problems are for example the difficulty to reconstruct highly strained objects, and determining a good metric that would allow scientists to trust reconstruted data.

\subsection{The Jupyter environment}

The Jupyter Notebook environment \parencite{IPython, Kluyver2016jupyter} was chosen for the development of data analysis tools during this thesis for its versatile, user-friendly and browser-based interface.
Notebooks can be used to take notes during experiments or to create custom \textit{Python} functions.
They can be shared in the \textit{.ipynb} format or as \textit{.pdf} documents.
Moreover, Jupyter Notebook has proven to be an effective tool for the analysis of synchrotron data, in terms of graphical user interface (GUI) \parencite{Martini2019a,Simonne2020}, but also in terms of supporting scientific communities looking for high-performance frameworks \parencite{jupyter_computing_4, jupyter_computing_1, jupyter_computing_3, jupyter_computing_2, 9307800}.

Large scale facilities and institutions seek ways to provide remote-access to high performance computing services for their users, which combine existing solutions in an interactive and user-friendly environment.
Indeed, to simplify the data analysis pipelines in fourth-generation synchrotrons, it is of critical importance to offer the possibility for external users to analyse the collected data remotely with access to computational environments.
Jupyter (https://jupyter.org/) is particularly advantageous and was chosen by several institutions for such purposes, \textit{e.g}. Google (Google Colab), the EGI federation, or the European Synchrotron.

Computational environments and resources as well as remote access to Jupyter Notebook or JupyterLab is provided by JupyterHub, a specific interface for computing clusters that offers the possibility to proceed to heavy computations without relying on specific hardware, \textit{e.g.} graphical processing units (GPUs), mandatory for accelerated phase retrieval with \textit{PyNX}.
The sharing of high performance computing clusters ressources is handled by SLURM (Simple Linux Utility for Resource Management).

Moreover, researchers can create their own work-spaces, with direct access to tailored computational environments, without having to install multiple software, keeping in mind that the use of GPUs for scripts optimisation is far from accessible.
Thus, system administrators can efficiently manage complex environments accessible to all users.
Finally, remotely accessing the data avoids data storage issues, which can quickly become problematic with current experiments.
Note that with the latest version of Jupyter, Jupyter4, real-time collaboration (RTC) will be supported, which also pushes Jupyter Notebook forward as a tool for laboratory notebooks.

% For most of the GUIs, interactive data analysis/visualisation relies on the \textit{ipywidgets} library.
% Widgets are represented in the back-end by a single object linked to a single parameter.
% The front-end relies on \textit{JavaScript} code; each time a widget is displayed, a new representation of that same object is created.
% The widgets' style, orientation, and layout attributes can be edited to customize the final window, \textit{e.g.}, the layout attribute exposes a number of properties that impact how widgets are laid out, such as height and width.

\subsection{Thorondor} \label{sec:Thorondor}

Due to the lack of available free data reduction and analysis software at the B07 beamline in the Diamond synchrotron, a modified version \parencite{Simonne_Thorondor_2022_Diamond} of Thorondor \parencite{Simonne2020}, a program originally written for the analysis of NEXAFS data, was used to process the XPS data.
To facilitate data reproducibility and open-source analysis workflows, an example of the data recorded during this thesis is available directly in the GitHub repository \cite{Simonne_Thorondor_2022_Diamond}.

Since the XPS measurements were performed at different total pressures, the raw data had first to be reduced in order to analyse the different spectra in a systematic way.
The classical workflow for the analysis of XPS data is to first align the recorded spectra on the Fermi edge that corresponds to the kinetic energy of the first electron that escapes the sample.
By doing so, one can be confident that any shift in the peak positions is due to chemical changes, such as the oxidation state of the sample, and not to charging effects of the sample (cite).

Secondly, to be able to quantify and compare the evolution of the peak intensity, one must normalize the intensity of the detected electron beam since the electron mean free path depends on the pressure in the reaction chamber (cite).
The range of kinetic energy just before the absorption edge of Pt 4f was chosen since it had the best signal to noise ratio and does not depend on any experimental parameter besides the pressure.

Finally, for the peaks that showed a good signal to noise ratio, the fitting of the peak shape was realised thanks to the \textit{lmfit} \parencite{Newville2016} package by the means of the Doniach-equation which is the best approximation of the asymmetric peak shape resulting from the convolution of the analyser function and the photoelectron process in metals \parencite{Doniach_1970}.

\subsection{Gwaihir} \label{sec:Gwaihir}

BCDI relies on iterative algorithms to solve the phase lost during the measurement (\cite{robinson_coherent_2009} - sec. \ref{sec:BCDI}).
A 3D intensity distribution in the vicinity of a Bragg peak (stack of diffraction patterns forming a 3D reciprocal space map with the proper sampling) is collected from a sample illuminated with coherent light \parencite{robinson_coherent_2005}, and serves as input for phase retrieval.

\begin{figure}[!htb]
    \includegraphics[width=\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/Packages.png}
    \caption{Flow chart illustrating the main steps in the BCDI data analysis workflow. \textit{Gwaihir} links the \textit{bcdi} and \textit{PyNX} packages \textit{via} its graphical user interface and command line scripts, resulting in a complete and easily understandable data analysis workflow. Opt. stands for optional.
    }
    \label{fig:Packages}
\end{figure}

There are three main steps to image the strain (fig. \ref{fig:Packages}); it must first be pre-processed (removal of parasitic scattering intensities, \textit{etc.} \parencite{ozturk_performance_2017}).
It must then be inverted \textit{via} phase retrieval \parencite{miao_possible_2000}, the phase containing important information lost during the measurement.
Finally, it is possible to extract meaningful results from the three dimensional complex images after post-processing (removal of phase offset, interpolation in a common orthonormal frame, \textit{etc}).
Several software packages were developed to solve these steps but none bring a comprehensive pipeline from start to finish.

For example, \textit{PyNX} \parencite{pynx2020operators} focuses on the phase retrieval step, \textit{bcdi} \parencite{jerome_carnis_2021_5741935} focuses on data pre-processing and post-processing, and \textit{Cohere} \parencite{cohere_2021} focuses on pre-processing and phase retrieval.
A few graphical user interfaces (GUIs) also exist, such as \textit{Cohere} \parencite{cohere_2021}, \textit{Phasor} \parencite{dzhigaev_dzhigaevdphasor_2021}, and \textit{Bonsu} \parencite{newton_bonsu_2012}.
Providing a workflow will reduce the time spent on data analysis for newcomers, and improve results reproducibility by facilitating sharing while keeping track of analysis parameters and metadata.

\textit{Gwaihir} is a tool developed during this thesis which brings together the functionality of the \textit{PyNX} package for phase retrieval and \textit{bcdi} package for the pre- and post- processing in a graphical user interface (GUI) built for the Jupyter framework \parencite{Kluyver2016jupyter}.
It provides an interface to the bleeding edge of data analysis in BCDI, it can be used locally or remotely and offers an interactive and user-friendly interface with complex functionality satisfying both beginners and experts.

\subsubsection{Software structure}

A part of the BCDI community relies on \textit{Python}, an accessible language that has gradually become one of the most popular, versatile \parencite{IPython, Newville2016} and widely-taught \parencite{Scopatz2015, McKinney2017, Boulle2019} programming languages in science.
\textit{Gwaihir} followed this initiative by regrouping data visualisation tools, workflow guidelines, and a user-friendly interface around two \textit{Python} packages (\textit{PyNX} and \textit{bcdi}), that together, offer a complete data analysis suite (fig. \ref{fig:Packages}).

\textit{Gwaihir} works with \textit{Python} 3.9 and is licensed under the GNU General Public License v3.0.
The source code as well as the latest developments are available on GitHub while each stable version will be released on the \textit{Python} Package Index (PyPi), along with its documentation.

\textit{bcdi} \parencite{jerome_carnis_2021_5741935} tackles the pre-processing and post-processing of the BCDI data (fig. \ref{fig:Packages}).
Aside from the specific details of the experimental setup (diffractometer and setup geometry, detector type, file system), the majority of the pre-processing pipeline is actually beamline-independent.
Based on this observation, \textit{bcdi} leverages inheritance and transforms the raw data to a common data format used for phase-retrieval.
It frees the user from having to learn or remember the technical details for each beamline and sets a common strategy across beamlines.

\textit{PyNX} \parencite{pynx2011} is a toolkit with assorted \textit{Python} modules and command-line scripts which can be used for the analysis of coherent X-ray imaging data, including phase retrieval (fig. \ref{fig:Packages}).
All calculations can be executed and distributed on multiple graphical processing units (GPUs) for accelerated computing using MPI.
Elementary algorithms can be easily tailored, built upon, and combined using an operator-based approach, allowing full flexibility with high-performance computing \parencite{pynx2020operators}.
Some facilities have started to make computational resources available to their users; those machines are an ideal environment for \textit{PyNX} which is nowadays available through SLURM, at the ESRF - The European Synchrotron, or GRADES at the Optimized Light Source of Intermediate Energy of LURE (SOLEIL).

\subsubsection{Workflow for Bragg coherent diffraction imaging} \label{sec:Workflow}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.7\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/Workflow.png}
    \caption{Workflow steps taken in \textit{Gwaihir}; the circular workflow illustrates data reproducibility, a key concept, facilitated by using the \textit{cxi} architecture.}
    \label{fig:Workflow}
\end{figure}

\textit{Gwaihir} offers an interactive workflow separated in three main groups, data pre-processing, phase retrieval, and data post-processing (fig. \ref{fig:Packages}, \ref{fig:Workflow}). It is meant to be reproducible, simple to share, resulting in the phase and amplitude of the probed object. To illustrate this, the results of the following procedure on a data-set collected at the P10 beamline in at PETRA III are shown in fig. \ref{fig:GUI_file} (CXI dataset ID 195).

The parameters used during the analysis are displayed in the GUI in an order following a typical workflow, to underline the evolution of the data processing.
These parameters are ultimately saved as attributes of the \textit{Python} data object, to keep track of the actions applied to the raw data to obtain the final result.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.49\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/CropDiffPatternRed.png}
    %\includegraphics[width=\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/CropObjectAmplutideRed2.png}
    \includegraphics[width=0.49\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/crop_phase_red.png}
    \caption{
    %Each parameter used during the data analysis is stored in the same \textit{.cxi} file, along with the results, as seen in the nested architecture on the left of the Figure.
    Here a 2D slice of a 3D coherent diffraction pattern is shown (top), with the phase of the electron density (radiants) of the reconstructed object in which the facets are clearly visible (bottom). The data array are displayed \textit{via} \textit{JupyterHub}, a browser-based data analysis platform available on the \textit{SLURM} computer cluster for the ESRF and the \textit{GRADES} computing cluster at SOLEIL.
    }
    \label{fig:GUI_file}
\end{figure}

\subsubsection{Pre-processing}

Data pre-processing aims at improving the quality of the phase retrieval by optimising the size and content of the 3D array used as input.
The minimal processing consists in loading the raw data and stacking it together as an input for the phase retrieval.
Intermediate optional steps can be added \textit{via} a YAML (YAML Ain't Markup Language) configuration file.

Once the raw data is collected, different pre-processing parameters can be modified to optimize the 3D diffraction intensity.
For example, the array must be centered for the Fourier transforms, the center is either fixed manually as the center of the Bragg peak if known, or determined as the center of mass of the 3D array during the analysis.
The array can also be cropped to reduce its size and decrease the computing time during phase retrieval by removing the points furthest from the Bragg peak where the signal-to-noise ratio is low (fig. \ref{fig:GUI_file}).

It is important to create a detector mask prior to the experiment to correct the raw data for hypothetical hot-pixels and background.
Moreover, it is also possible to correct the raw data frame by frame for spurious data which taints the diffraction pattern.
For example, it is possible, while recording the 3D diffracted intensity of a given reflection, to have signal coming either from the substrate or from neighbouring objects that will be summed to the probed object's intensity.

Finally, it is possible to normalise the raw data by an intensity monitor, or compute the scattering vector $\vec{q}$ of the measurement, from the instrumental geometry and parameters.

\subsubsection{Phase retrieval}

The diffracted intensity collected by the detector is proportional to the squared modulus of the structure factor $F(\vec{q})$ (Equations \ref{eq:ScatteredIntensity}-\ref{eq:DetectorIntensity}).
To retrieve the phase from the diffracted intensity, \textit{PyNX} uses iterative algorithms (sec \ref{sec:BCDI}).
New methods such as convolution neural networks (CNN) are under study and have started to show some encouraging results, but are not yet sufficiently robust, particularly in the case of strained particles \parencite{cherukara_real-time_2018,chan_rapid_2021,Wu2021a}.

The parameters necessary to phase retrieval \parencite{fienup_phase_1982,fienup_reconstruction_1978,Marchesini2007,pynx2020operators} \textit{e.g.} the support threshold, the number of iterations for each algorithm, the object initialisation procedure (square, sphere, auto-correlation), and so on can be modified in the GUI (fig. \ref{fig:PRT}).
Initial guesses are given for each parameter but must be refined by the user to optimize the results.

\textit{PyNX}'s recent update includes mathematical operators \parencite{pynx2020operators} which represent most of the reconstruction operations traditionally used in phase retrieval \parencite{gerchberg_practical_1972,fienup_reconstruction_1978,Marchesini2007} and is at the foundation of quick and interactive phase retrieval in \textit{Gwaihir}.
Indeed, state-of-the-art GPUs available onsite where data collection occurs enable almost real time data reduction.
This results in quick visualisation of the probed object's amplitude and phase offering the possibility to synchrotron users to optimize their data acquisition during the experiment.
Such live feedback is critical to the success of an experiment and provides broader opportunities in the type of experiments that can be carried on.

It is possible to refine the input parameters directly in the GUI by visualizing their impact on the reconstructed object.
This allows to refine the phase retrieval input parameters before submitting a batch job, that will spawn a sub-process on the computing cluster for phase retrieval.
With a single click, several dozens of solutions can be computed in the matter of minutes on computing clusters.

With well-tuned parameters and high quality data-sets, phase retrieval converges towards the same solution, but with minor differences between each reconstructed object, related to the phase retrieval process.
The most important limiting factors for the convergence of the reconstruction algorithms are the quality of the measurement and the amout of strain going through the particle.
A crystal which is well faceted and not strained will always converge towards the same solution.
However, in the case of highly strained object, part of the intensity will be scattered in a direction that does not fit the area collected by the detector which will result in void areas inside the reconstructed electronic density.
Moreover, the support becomes very difficult to determine since the diffraction intensity does not correspond to the lattice factor anymore.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/phase_retrieval_tab.png}
    \caption{Phase retrieval tab in \textit{Gwaihir}. Parameters are separated into groups (files, support, point-spread function, algorithms, ...) and detailed in the README tab. The object can be reconstructed through a batch job, submitted to the computing cluster in the backend, or with operators, that will plot the evolution of the reconstruction object in the Notebook.}
    \label{fig:PRT}
\end{figure}

\textit{Gwaihir} provides a wide range of selection criteria to find the best solution.
Each reconstructed object has a list of final attributes that can be used as a criteria for selection, such as the free log-likelihood \parencite{pynx2020FLLK} or the standard deviation of the modulus of the reconstructed object.
In the case of crystallographic defects, specific metrics (Chi, Sharp, Max volume, ...) were derived that perform best depending on the type of defect in the object \parencite{Ulvestad2017}.
Following the selection criteria, it is possible to quickly identify the solutions of poor quality that must be ignored to create a set of best solutions.
\textit{PyNX} then offers a method that merges this set into a single solution by computing eigen-vectors for the selected solutions \parencite{pynx2020FLLK}.
An alternative approach also available in \textit{Gwaihir} is to take the average of the best solutions \parencite{ulvestad_nanoscale_2014}.

\subsubsection{Post-processing}

Post-processing regroups methods applied to the complex output of the phase retrieval.
Once the solution with the best figure of merit is selected, it is possible to use the \textit{bcdi} scripts to process the object's phase.
If the data is still in the detector frame (geometric transformation not applied during pre-processing), the data is interpolated in an orthogonal frame using the transformation matrix.
First, the correct beamline and instrumental parameters must be selected (\textit{e.g.} sample-detector distance, probing energy, detector pixel size, \textit{etc}...), usually constant throughout the experiment.
For example, the orthogonalisation tab regroups the parameters needed to properly set up the transformation matrix between the sample frame and \textit{e.g.} the laboratory frame.
By choosing to transform the final object in a common, orthogonal frame, it becomes easier to compare the object's evolution when probing different Bragg reflections \parencite{crystal_sarah}.
The geometric transformation can be realized using either the transformation matrix \parencite{thesismark} or the existing  \textit{xrayutilities} package \parencite{kriegner_xrayutilities_2013}, depending on which reference basis is needed.
After phase unwrapping, a refraction and absorption correction are optionally applied, and possible phase ramp and phase offset are removed.
At this point the displacement and the strain component are calculated from the phase.
The phase origin can first have an impact when comparing the lattice displacement between different reconstructions \parencite{Atlan2023}.
In the case of weak strain, it can be sufficient to consider the center of mass of the object as the origin of phase.
However, this can become quite complex in the case of defects or defaults in the object.
Special methods are defined to target this issue in \textit{bcdi}.
For example, \cite{guizar-sicairos_phase_2011} and \cite{hofmann_nanoscale_2020} proposed a convenient method for the numerical calculation of phase gradients in the presence of phase jumps.

\subsubsection{Graphical interface}

\textit{Gwaihir} links in a unique user-friendly and interactive GUI the aforementioned packages whilst offering 2D/3D browser-based data visualisation tools.
Therefore, the main purpose of \textit{Gwaihir}, is to offer a possibility for the user to work solely in the Jupyter environment.
The graphical user interface is divided into 10 tabs, separated into three groups: instrumental parameters, data processing and data analysis.
The aim of this organization is to achieve a comprehensible but fluid workflow, whilst still separating each step.
A final tab contains information about the different methods and parameters used in the workflow, as well as a tutorial on the GUI.

% \begin{figure}[!htb]
%     \centering
%     \includegraphics[width=\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/GUI_tab1.png}
%     \caption{Screenshot of \textit{Gwaihir} graphical user interface displayed in Jupyter Notebook. Jupyter Notebook can be used on computing clusters via JupyterHub or on any machine for local use.}
%     \label{fig:GUI_window}
% \end{figure}

% During the manipulation of the GUI, it is possible to quickly produce a final result thanks to those intuitive and interactive widgets, whose values are passed as arguments to the data analysis functions (fig \ref{fig:3D_object}).
% For example, to designate the type of detector used, a value must be selected in a dropdown list that comprises the following options: Eiger2M, Maxipix, Eiger4M, Merlin, Timepix.
% These correspond to the different detectors currently supported in the \textit{BCDI} package.
% The final interface is created by setting the different widgets on a grid, with the output of different functions printed below.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/3D_obj.png}
    %\includegraphics[width=\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/3D_peak.png}
    \caption{
    Widgets can be used to select folders and files, but also to tune the values of input parameters in functions.
    The selection of the data array, the colormap, and the contour of the resulting 3D object are performed through different widgets.
    }
    \label{fig:3D_object}
\end{figure}

Jupyter natively offers multiple options for interactive data plotting.
Most of the figures displayed in the GUI are based on the \textit{matplotlib} package \parencite{hunter2007matplotlib}.
For example, it is possible to select a list of 3D complex data arrays and to visualize 2D slices in each dimension of their amplitude or phase, with different colour-maps (diverging, sequential, cyclic) and scale (linear, logarithmic).

Moreover, an interactive 3D visualisation tool is provided that relies on the \textit{ipyvolume} library \parencite{ipyvolume}, itself built on top of \textit{ipywidgets}, and specifically designed to quickly render large 3D data arrays.
In the specific case of single volume rendering for the reconstructed object, the object's surface is defined by a threshold of its maximum density (fig. \ref{fig:3D_object}).
The object surface can be colour-mapped with the values of the displacement and strain retrieved during the data analysis.

To further interact with the figures, (\textit{e.g.} zoom, set the colorbar range), tools were implemented that rely on \textit{Bokeh} \parencite{Bokeh}, a \textit{Python} library that transforms figures in interactive web-pages (fig. \ref{fig:BokehDetector}), that can also be displayed in Jupyter Notebook.

To resume, the GUI regroups data reduction, analysis and visualisation tools in the Jupyter Notebook interface \textit{via} interactive methods.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.66\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/detector_slice_bokeh.png}
    \caption{Detector images can be viewed interactively with \textit{Bokeh}, to zoom on the data, and visualize, for example, the intensity collected on each pixel.}
    \label{fig:BokehDetector}
\end{figure}

\subsubsection{Command line scripts}

The complete workflow for data processing can also be launched from the command-line using \textit{Python} scripts, whereas the link between each package is based on \textit{BASH} scripts.
This approach is both fast and versatile, designed to quickly iterate on several data-sets to test parameters values, but less intuitive compared to the interactive GUI.

A set of default parameters stored in a configuration file is used, that can also be overwritten by directly providing keywords directly in the command line, \textit{e.g.} the scan number.
The configuration files are written in \textit{YAML} (fig. \ref{fig:YAML_file}).
One may still manually mask the data or manually select the best reconstruction before post-processing.
This option allows a quick visualisation of the probed object.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/yaml.png}
    \caption{
    Configuration file in YAML, a human-readable data-serialization language with a minimalist syntax.
    Each parameter used for the analysis methods are stored.
    A configuration file is generated for the pre-processing and post-processing scripts, as well as for the phase retrieval.
    }
    \label{fig:YAML_file}
\end{figure}

\subsubsection{Data storage}

Already, raw and processed data can be accessed for the BCDI technique through the \textit{.cxi} database (\url{https://cxidb.org/}) \parencite{Maia2012}, which aims at creating a single data-storing architecture/format for coherent X-ray imaging experiments.
The data input/output follows Nexus definitions \parencite{Konnecke2015} built in a CXI format hdf5 file.
Both clarity and consistency in data formatting encourages the reproducibility of the science produced (see https://www.panosc.eu/) and guarantees the workflow.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.7\textwidth]{/home/david/Documents/PhD/Figures/gwaihir/tree.png}
    \caption{
    Each parameter used during the data analysis is stored in the same \textit{.cxi} file, along with the results, in a  nested architecture.
    "instrument\_1" regroups parameters associated with the instrumental setup, "data\_1" is the collected diffraction pattern.
    "image\_2" regroups the parameters associated with phase retrieval; "data\_2" is the reconstructed Bragg electronic density chosen for post-processing.
    "image\_3" regroups parameters linked to data processing, as well as the processed amplitude, phase and resulting strain. "data\_3" is a link to the processed phase of the object.
    The file structure is displayed \textit{via} \textit{JupyterHub}, a browser-based data analysis platform available on the \textit{SLURM} computer cluster for the ESRF and the \textit{GRADES} computing cluster at SOLEIL.
    }
    \label{fig:TREE}
\end{figure}

Since not all beamlines provide self-explaining NeXuS data-sets, it is the \textit{bcdi} package together with \textit{xrayutilities} \cite{kriegner_xrayutilities_2013} that allows the support of the coherent imaging beamlines ID01 (ESRF), P10 (PETRA III), SixS and CRISTAL (SOLEIL), NanoMAX (MAX IV) and 34-ID-C (APS).
Data pre-processing will generate two files stored as NumPy arrays \cite{NumPy}, corresponding to the diffraction intensity and mask.
These two files are then used for phase retrieval, for which the final object is saved in a \textit{cxi} file, later used for data post-processing.
In the case of simulation, the simulated diffraction intensity can be stored as a NumPy array to start the workflow from phase retrieval.

In \textit{Gwaihir}, data sharing across teams and team members is facilitated by the creation of a single output file, respecting the \textit{CXI} \cite{Maia2012}, and thus the NeXuS \cite{Konnecke2015} architectures.
As written in \cite{Konnecke2015}, authors of data-reduction and data-analysis software can use NeXus to store processed data along with metadata and a processing log.
This is illustrated in fig. \ref{fig:TREE}, in which the \textit{cxi} data-set tree is displayed.
The raw data, along with all the parameters associated to the analysis, and the final results are stored, allowing complete data analysis reproducibility (fig. \ref{fig:TREE}).

Key parameters for data reproducibility are the transformation matrix used for the final interpolation, the voxel size of the resulting 3D array, the probed reciprocal space range after data pre-processing $(\delta q_x , \delta q_y , \delta q_z)$, the iso-surface threshold, \textit{etc}.
By sharing those parameters along with the diffraction intensity and complex Bragg electronic density, there is a push towards more transparency in the methods used to obtain the final results.
Comments or metadata, such as the horizontal and vertical coherence lengths, or beam size, if determined prior to the experiment, can also be saved.
The aim is first to have a comprehensible architecture, and secondly to be able to reproduce anyone's result from this file.
On a small scale, results are easier to share between collaborators and more understandable, while on a larger scale peer-review is facilitated.

\subsection{BINoculars} \label{sec:BINoculars}

BINoculars \parencite{Roobol2015} is a data reduction software used first to concatenate together the scattering intensity from successive angular scans in the same region of the reciprocal space, and secondly to optionally change the data's reference frame from the angular space to another.
Those two steps can be performed simultaneously by assigning to each voxel of the 3D scattered intensity a position in the new frame.
From subsequent scans, it is possible that each position in the new frame was scanned multiple times, meaning that there were multiple contributions to the total scattered intensity at that position.
The final intensity is therefore the division of the total scattering intensity in each voxel by the number of contribution to that voxel (eq. \ref{eq:BinocularsIntensity}).
If there are no contributions, the intensity is set to NaN (Not a number).

\begin{equation}
    \label{eq:BinocularsIntensity}
    Intensity =
        \begin{cases}
            counts \, /  \,contributions  & \text{if contributions $\neq$ 0} \\
            NaN & \text{if contributions = 0}
        \end{cases}
\end{equation}

Different orthonormal frames can be of interest when analysing the data, the q-space ($q_x, q_y, q_z$) is useful to index Bragg peaks and to access the related inter-reticular spacing $d_{hkl}$.
It also provides a non-distorted view of the reciprocal space in contrast with the ($h, k, l$) frame which directly depends on the crystal's Bravais lattice.
Different lattices can be used to highlight the difference of geometry between the bulk structure and the surface structure (fig. \ref{fig:MapExampleBinoculars}).

\begin{figure}[!htb]
    \includegraphics[width=0.49\textwidth]{/home/david/Documents/PhD/Figures/sxrd_data/Pt100/maps/Map_hkl_surf_or_2227-2283_patched.png}
    \includegraphics[width=0.49\textwidth]{/home/david/Documents/PhD/Figures/sxrd_data/Pt100/maps/Map_hkl_hex_or_2227-2283_not_patched.png}
    \caption{
    Reciprocal space in-plane map projected on $l$.
    The large high intensity peaks are Bragg peaks from the bulk structure, the low intensity peaks are surface reconstructions.
    The left map is computed using a cubic unit cell, two hexagonal unit cell corresponding to the surface reconstructions' structure are drawn as diamonds.
    The right image is computed using the red diamong hexagonal unit cell that corresponds to the structure of one phase of the surface reconstructions.
    }
    \label{fig:MapExampleBinoculars}
\end{figure}

When analysing surface x-ray diffraction data, the in-plane maps in reciprocal space are usually projected perpendicular to $l$ (or $q_z$) to show possible Bragg peaks from surface reconstruction (fig. \ref{fig:MapExampleBinoculars}) and therefore contain the sum of the scattering intensity in a small layer $\delta l$ (or $\delta q_z$).
It is of utmost importance to select a thin layer to not drown the surface signal with the background or bulk signal.

BINoculars also contains a sub-module (\textit{fitaid}) that allows the integration of the scattered intensity in the reciprocal space as a function of one axis.
Usually, to study the evolution of crystal truncation rods, the data is projected either in the ($h, k, l$) or ($q_x, q_y, q_z$) frame and then integrated in a square area around a Bragg peak as a function of $l$ or $q_z$.
The voxel size in each direction is set through a parameter, which raises a compromise between array size, resolution, and an interpolation process.
When in the ($q_x, q_y, q_z$) space, it is possible to go as low as \qty{0.005}{\angstrom^{-1}} in each direction, the limit being set by instrumental parameters such as the detector pixel size, the amount of angular scans performed, the counting time or the number of iteration per scans.
If the number of iteration is too low for the desired resolution, the process will yield an image with empty areas corresponding to voxels where no data could be recorded.
This is often the case during the collection of crystal truncation rods, at high values of $l$, since the region probed by the detector becomes thinner in $l$ with every iteration \parencite{Drnec2014}.
To avoid having to rely on an interpolation to fill these voxels, it is better to use a larger resolution when concatenating the images, and then to use the same step when integrating the data.

The intensity integrated \textit{via} \textit{fitaid} also contains the background signal.
To quantitatively compare the integrated intensity with simulated structure factors, it is mandatory to subtract the background intensity from the total intensity.
The background is calculated by taking the sum of all the values of selected background regions, around the CTR, corrected by the number of voxels in those area.
The average background value per voxel is then subtracted to each voxel in the CTR region of interest (ROI), the structure factor $F$ can then be computed as follows:

\begin{equation}
    |F|^2 \propto I_{roi} - \frac{N_{roi}}{N_{bkg}} \times I_{bkg}
\end{equation}

$I_{roi}$ is the integrated intensity in the ROI, $N_{roi}$ the number of voxels in the ROI, $I_{bkg}$ the integrated background intensity and $N_{bkg}$ the number of voxels in the background.
% This becomes more complicated when there are additional signals such as powder rings that cross the CTR at different $l$ values.

A very fine resolution will result in extremely large arrays that can be difficult to manipulate on low ressources computer, for example the maps shown in fig. \ref{fig:MapExampleBinoculars} have a three dimentional shape equal to (\numproduct{560 x 953 x 39}) which results in \num{2.1e7} voxels in the array.
Originally, this corresponds to the concatenation of 40 angular scans, each with 1010 steps, \textit{i.e.} containing 1010 2D detector images that have a shape equal to (\numproduct{240 x 560}).
This represents \num{5.5e9} pixels to process together to create such in-plane maps.
High performance computing clusters are therefore needed for the reduction of surface x-ray diffraction data with BINoculars, the data analysis is performed in a second step with programs such as ROD \parencite{Vlieg2000}.

\subsection{ROD} \label{sec:ROD}

The primary challenge associated with surface x-ray diffraction lies in the complexity of data analysis, as researchers are limited to a small portion of the reciprocal space due to the extended data acquisition time \parencite{Gustafson2014}.
On one hand, a fine sampling of the reciprocal space will take hours which is hardly compatible with time-resolved experiments.
On the other hand, skipping areas of the reciprocal space could lead to loss of critical information if the symmetry of the surface structure is not known prior to the experiment.

SXRD data analysis consists of finding the 3D ($\vec{a}, \vec{b}$) surface unit cell structure from in-plane maps and crystal truncation rods by comparing the position and intensity of Bragg peaks with simulated peaks.
For high symmetry structures, a small portion of the reciprocal space can sometimes be sufficient to have a final solution.

In a second step, it is possible to refine the position of each atom in the unit cell to take into account possible strain along $\vec{c}$ that will have a significant effect in the shape of the crystal truncation rods.
The structure along $\vec{c}$ is refined by computing the shape of crystal truncation rods and comparing them with experimental data, the more rods were measured and the higher the extent of each rod in $l$, the more accurate the final structure.

ROD is a computer program written in \textit{C} \parencite{Vlieg2000} that is used both to simulate structure factors from surface structures and to refine surface structure by fitting the crystal truncation rod intensity.
Since ROD is written in \textit{C} and does not come with a graphical user interface, alternatives have been developped such as GenX \parencite{Bjorck2007,Glavic2022} written in \textit{Python} which mostly focuses on reflectometry.
In the frame of this thesis, ROD was chosen for the analysis of SXRD data.
