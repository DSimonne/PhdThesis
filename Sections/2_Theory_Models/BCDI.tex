\section{BCDI}

\subsection{Coherent diffraction}

Bragg coherent diffractive imaging (BCDI) is a lensless x-ray imaging technique that uses computational algorithms in place of physical lenses to achieve high-resolution imaging. It can be used to visualize the Bragg electron density and atomic displacement fields of crystalline materials in three-dimensional (3D) detail and with nanometer resolution.

We focus only on elastic scattering as that is the main process exploited when studying the structure of materials (the x-ray photon is elastically scattered (energy is conserved) and both the incident and scattered photons have the same wavelength).

By adding up the scattered amplitudes of an arrangement of atoms, we can then get the diffracted amplitude for a crystal. Note that the following derivations only apply if the diffraction pattern is viewed at a distance far away from the diffracting object. This region is known as the far-field or Fraunhofer region.

\subsection{Scattering by strained crystals}


\subsection{Phase retrieval}

To understand the phase problem, it's important to know that when X-rays interact with a crystal, they are diffracted by the crystal lattice structure. The diffracted X-rays form a complex interference pattern of constructive and destructive interference, resulting in a series of spots known as diffraction peaks.

The intensities of these diffraction peaks can be measured experimentally, providing valuable information about the distribution of electrons within the crystal. However, the phases of the diffracted waves are not directly determined from the intensity data.

In order to obtain a complete and accurate picture of the electron density within the crystal, both the amplitudes and phases of the diffracted waves are needed. The problem lies in determining these phases, which is often challenging or even impossible to do directly from experimental measurements.

The phase problem arises because the intensities of the diffracted waves are obtained through measurement, but the phases are typically missing. Without the phase information, it is impossible to directly reconstruct the electron density distribution and obtain a detailed structural model of the crystal.

Basic algorithm is Error-Retrieval (ER), quick but can converge towards local minimum. 
The support corresponds to a shape when the object is included. The density of the object is thus equal to zero outside the support. (Fienup, 1978).

In practice, a slow convergence of the ER algorithm is often observed. The error-metric does not evolve and the algorithm is sort of stuck in a local minimum.

To overcome the problem of stagnation in local minima from ER, (FIenup, 1982) introduced the Hybrid-Input-Output algorithm, that differs in its application of real space constraints. Feedback parameter $\beta$, 
In practice, this adaptation is efficient and significantly enhances the convergence speed. It can be seen as a little perturbation that allows to leave a local minimum.


However, the HIO algorithm still fails sometimes, and this explains why the ER and HIO algorithms are generally used in combination.

\subsubsection{Oversampling}
Oversampling in Fourier transforms refers to the practice of sampling a signal at a higher sampling rate than the Nyquist rate, which is twice the highest frequency present in the signal. By increasing the sampling rate, more samples are taken per unit of time, resulting in a denser set of data points.

When it comes to Fourier transforms, oversampling can offer several advantages:

1. Increased frequency resolution: Oversampling provides finer frequency resolution in the frequency domain. The additional samples capture more detailed information about the signal's frequency content, allowing for better analysis and representation of high-frequency components.

2. Reduced spectral leakage: Spectral leakage occurs when the frequency components of a signal spread into neighboring frequency bins in the Fourier transform. By oversampling, the spectral leakage effect can be minimized since the frequency bins become narrower, making it easier to distinguish and analyze closely spaced frequencies.

3. Improved interpolation and reconstruction: Oversampling can enhance the accuracy of interpolation and signal reconstruction from the Fourier domain back to the time domain. With more densely spaced samples, the interpolation process can more faithfully reconstruct the original signal without introducing artifacts.

4. Mitigation of aliasing effects: Aliasing occurs when high-frequency components of a signal are mistakenly represented as lower frequencies due to insufficient sampling. Oversampling helps to reduce aliasing by capturing more data points, allowing for a more accurate representation of the original signal's frequency content.

5. Enhanced filtering and noise suppression: Oversampling can provide improved filtering capabilities and noise suppression. With more data points available, it becomes easier to design and apply filters in the frequency domain, effectively attenuating unwanted frequency components and reducing noise interference.

It's important to note that oversampling comes at the cost of increased computational requirements and data storage. More samples mean more data to process and store, which can impact the efficiency and memory requirements of Fourier transform operations.

In summary, oversampling in Fourier transforms involves sampling a signal at a higher rate than the Nyquist rate, offering benefits such as increased frequency resolution, reduced spectral leakage, improved interpolation and reconstruction, mitigation of aliasing effects, and enhanced filtering and noise suppression. It is a technique commonly used when higher accuracy and detailed frequency analysis are desired in applications involving Fourier transforms.

\subsubsection{Fast Fourier transform}

The Fast Fourier Transform (FFT) algorithm is a widely used computational technique for efficiently computing the discrete Fourier transform (DFT) and its inverse.
The DFT is a mathematical operation that transforms a time-domain signal into its frequency-domain representation, revealing the spectral content of the signal.

The FFT algorithm was developed by Cooley and Tukey in the 1960s and revolutionized the field of digital signal processing due to its significant speed improvement over the conventional DFT calculation.

Simplified explanation of the FFT algorithm:

1. Input: The algorithm takes as input a sequence of N complex numbers, representing a discrete-time signal.

2. Splitting: The sequence is divided into two halves, containing the even-indexed and odd-indexed elements. This splitting forms a butterfly pattern, where each element in the upper half interacts with a corresponding element in the lower half.

3. Recursive computation: The FFT algorithm recursively applies the DFT to each of the two halves. This process continues until the sequence size reduces to a base case, often a sequence of length 2.

4. Butterfly operations: At each recursion level, a series of butterfly operations are performed. In a butterfly operation, pairs of elements from the two halves are combined using twiddle factors (complex exponential factors) and summed. The twiddle factors rotate and scale the frequency components of the input signal.

5. Combining: As the recursion unwinds, the computed DFT values from the lower levels are combined to form the final DFT of the original input sequence.

The key concept behind the FFT algorithm is the exploitation of symmetry and periodicity properties of the complex exponential functions involved in the DFT computation. By using these properties and recursively dividing the sequence, the algorithm achieves a significant reduction in the number of operations required, resulting in a faster computation compared to the straightforward DFT calculation.

The FFT algorithm has a time complexity of $O(N log N)$, where N is the size of the input sequence. This is a significant improvement over the $O(N^2)$ complexity of the direct DFT calculation.

The FFT algorithm is extensively used in various applications, such as digital signal processing, image processing, audio processing, telecommunications, and many scientific and engineering fields that involve spectral analysis or frequency domain operations on discrete signals.

Overall, the FFT algorithm provides an efficient and powerful tool for transforming signals between the time and frequency domains, enabling a wide range of applications that require efficient spectral analysis and manipulation of digital signals.

\subsubsection{Support determination}
There are several techniques to estimate the support. In some cases, the shape and dimensions of the object have been already determined by other techniques (such as SEM or AFM for instance), and a support can be built from this knowledge. 

\subsubsection{Patterson function}
When the shape of the object is unknown, a rough estimate of the support can be obtained from the diffraction signal using the autocorrelation function (Marchesini 2003). It is based on the Patterson function which can be defined as the invert Fourier transform of the diffracted intensity. This function can be expressed as the convolution of the complex electron density.

The size of the crystal is overestimated by the Patterson function, since it provides its autocorrelation. In practice, a non uniform density leads to a non-trivial shape of the autocorrelation. The function needs to be threshold to start with a reasonable approximation. In most of the reconstructions in this manuscript, the threshold was set to 2\% of the maximum of the Patterson function. As discussed by Vaxelaire (2011), the method is not adapted to highly strained objects.

For large strain, the diffraction pattern has a large extent in the reciprocal space
(Beutier et al. 2013a). As a consequence, the Patterson function underestimates the size of the object, preventing any chance of success in the phase-retrieval procedure. In summary, if the shape and size of the object is unknown, it is not recommended to use the autocorrelation function as a first estimate of the support in the case of an highly strained system.

In combination of the ER and HIO algorithms, a third algorithm is routinely used for CDI. It is known as the shrink wrap (SW) algorithm and allows to update the support during the reconstruction. It was first introduced by Marchesini (2003) and has proven to greatly improve the convergence of the procedure.

In practice, the estimate is smoothed by convolution with a Gaussian. After convolution, a thresholding is applied to the smoothed image to a typical value of 10\% of the maximum value of the amplitude. Values above the threshold are set to 1 and values below are set to 0. The threshold is generally set to such low values to avoid to suppress too large parts of the support. Nevertheless, the convolution step allows to recover from a support that has been reduced

\subsection{Accessing strain and displacement}


\subsection{Computer programs}

\subsubsection{\textit{PyNX}}
Coherent X-ray imaging techniques developed during the last 20 years thanks to high brilliance in synchrotrons. Wide range of techniques:

\begin{itemize}
    \item Phase Contrast Imaging
    \item Coherent Diffraction Imaging, allowing to reconstruct single objects from their diffraction pattern alone, including strain imaging of crystalline nano-objects in the Bragg geometry.
    \item X-ray Ptychography, used in both near and far field regime, developed for imaging extended objects (larger than the incident beam), both in small angle and Bragg geometry, also usable in the Fourier regime by scanning the transmitted beam.
\end{itemize}
These techniques all provide high resolution 2D or 3D imaging, down to 5 to 15 nm resolution, depending on the instrumental setup.

Requires a coherent X-ray beam, readily available at synchrotrons facilities. Will benefit from upgrades of synchrotrons rings, which promises 2 orders of magnitude increase in the available coherent X-ray flux thanks to higher brilliance. This will enable faster dynamics and imaging experiments as well as reaching higher resolutions.
Higher energy coherent X-ray ($\>20 keV$) will enable data collection for thicker samples and allow to mitigate radiation damage with lower absorption.

PyNX has open-source coherent X-ray imaging modules \parencite{Favre-Nicolin2020}. All calculations for coherent imaging modules (\texttt{cdi, ptycho, wavefront}), respectively for Coherent Diffraction Imaging, Ptychography and coherent wavefront propagation (mostly used for simulation purposes) are executed on the GPU by using pyCUDA or pyOpenCL libraries. Language and GPU automatically selected based on tests.

Autocorrelation

The CDI technique consists of reconstructing an object from a far-field diffraction pattern alone, a technique which has been expanded to 3D reconstruction by collecting multiple ($>100$) projections around a rotation axis, either in the small angle, or in the Bragg geometries - the latter approach yielding information about strain in the reconstructed object \parencite{Li2020}.

In order to recover the object from non-redundant diffraction data, it is necessary to recover the lost phases of the measured amplitude (c.f. phase loss problem).

A variety of algorithms are available, all of which rely on alternating between a real-space estimate of the object and diffraction (Fourier) space, where an amplitude constraint can be applied from the measured intensity.


\subsubsection{\textit{bcdi}}

\subsubsection{Preprocessing}

Signal to noise ratio that influence FFT window size for cropping

Oversampling ratio

Poisson (Shot) noise 
